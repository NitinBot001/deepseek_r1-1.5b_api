name: AI Service Orchestrator

on:
  workflow_dispatch:
  repository_dispatch:
    types: [trigger-service]

jobs:
  service-instance:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Generate unique ID
      id: uid
      run: |
        UUID=$(uuidgen | cut -d'-' -f1)
        echo "instance_id=ai-$UUID" >> $GITHUB_OUTPUT

    - name: Setup environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Create Python application
      run: |
        cat > app.py <<EOL
        import os
        import requests
        from flask import Flask, request, jsonify
        from llama_cpp import Llama
        import sys

        app = Flask(__name__)
        MODEL_PATH = "model/gemma-2-2b-it-Q8_0.gguf"

        def download_model():
            os.makedirs("model", exist_ok=True)
            if not os.path.exists(MODEL_PATH):
                print("Downloading model...")
                r = requests.get("https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf", stream=True)
                with open(MODEL_PATH, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)

        download_model()
        llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=2048,
            n_threads=2,
            n_gpu_layers=0,
            verbose=False
        )

        @app.route('/chat', methods=['POST'])
        def chat():
            data = request.json
            prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{data.get('message','')}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n"
            output = llm(prompt, max_tokens=2048, stop=["<|eot_id|>"], temperature=0.5, top_p=0.9)
            return jsonify({"response": output['choices'][0]['text'].strip()})

        if __name__ == '__main__':
            app.run(host='0.0.0.0', port=5000)
        EOL

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flask requests llama-cpp-python

    - name: Install Node.js dependencies
      uses: actions/setup-node@v3
      with:
        node-version: '18'
    - run: npm install -g nport

    - name: Launch service cluster
      env:
        GH_PAT: ${{ secrets.GH_PAT }}
      run: |
        # Start Flask service
        python app.py > app.log 2>&1 &

        # Start tunnel
        npx nport -s ${{ steps.uid.outputs.instance_id }} -p 5000 > tunnel.log 2>&1 &

        # Wait for tunnel to establish and extract URL
        sleep 10
        TUNNEL_URL=$(grep -oP 'your domain is: \Khttps://\S+' tunnel.log)
        
        echo "Extracted Tunnel URL: $TUNNEL_URL"

        # Create instance.json file
        echo "{ \"tunnel_url\": \"$TUNNEL_URL\" }" > instance.json

        # Commit and push to the repository
        git config --global user.email "github-actions@github.com"
        git config --global user.name "github-actions"

        git clone https://x-access-token:$GH_PAT@github.com/NitinBot001/Audio-url-new-js.git repo
        cd repo

        mv ../instance.json instance.json
        git add instance.json
        git commit -m "Update tunnel URL to $TUNNEL_URL"
        git push origin main

        # Trigger next instance at 5.5 hours
        (sleep 19800 && 
        curl -X POST "https://api.github.com/repos/${{ github.repository }}/dispatches" \
          -H "Authorization: Bearer $GH_PAT" \
          -H "Accept: application/vnd.github.everest-preview+json" \
          -d '{"event_type": "trigger-service"}') &

        # Keep alive for full duration
        sleep 21600
