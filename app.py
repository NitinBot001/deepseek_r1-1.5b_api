from flask import Flask, request, jsonify
from llama_cpp import Llama
import os
import threading
import requests

# ЁЯФ╣ Flask App
app = Flask(__name__)

# ЁЯФ╣ Model Path (GitHub Secrets рд╕реЗ рд▓рд┐рдпрд╛ рдЧрдпрд╛)
MODEL_PATH = "model/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
# ЁЯФ╣ Context File URL
CONTEXT_URL = "https://raw.githubusercontent.com/NitinBot001/Unlimitedrdp/refs/heads/main/context.txt"
CONTEXT_FILE = "context.txt"

# ЁЯФ╣ Load Context File from GitHub
def download_context():
    try:
        response = requests.get(CONTEXT_URL)
        if response.status_code == 200:
            with open(CONTEXT_FILE, "w", encoding="utf-8") as file:
                file.write(response.text)
            print("тЬЕ Context file downloaded successfully!")
        else:
            print(f"тЪая╕П Failed to download context file! Status Code: {response.status_code}")
    except Exception as e:
        print(f"тЪая╕П Error downloading context file: {str(e)}")

# ЁЯФ╣ Load Context from File
def load_context():
    if os.path.exists(CONTEXT_FILE):
        with open(CONTEXT_FILE, "r", encoding="utf-8") as file:
            return file.read().strip()
    return ""

# ЁЯФ╣ Load Llama Model
print(f"ЁЯФД Loading model from: {MODEL_PATH} ...")
llm = Llama(model_path=MODEL_PATH, n_ctx=8192)  # ЁЯФе рдЬреНрдпрд╛рджрд╛ рдХреЙрдиреНрдЯреЗрдХреНрд╕реНрдЯ рд▓реЗрдВрде
print("тЬЕ Model Loaded Successfully!")

# ЁЯФ╣ Download Context on Startup
download_context()
CONTEXT_DATA = load_context()

# ЁЯФ╣ AI рдореЙрдбрд▓ рд╕реЗ рдЬрд╡рд╛рдм рдкрд╛рдиреЗ рдХрд╛ рдлрдВрдХреНрд╢рди
def get_ai_response(prompt):
    full_prompt = f"{CONTEXT_DATA}\n\nUser: {prompt}\nAI:"
    response = llm(
        full_prompt, 
        max_tokens=256, 
        stop=["\n", "User:"],  
        echo=False
    )
    return response["choices"][0]["text"].strip()

# ЁЯФ╣ API Route - рд╕рд╡рд╛рд▓ рдкреВрдЫреЛ, рдЬрд╡рд╛рдм рдкрд╛рдУ
@app.route("/ask", methods=["POST"])
def ask():
    try:
        data = request.get_json()
        question = data.get("question", "")

        if not question:
            return jsonify({"error": "Question is required!"}), 400

        print(f"ЁЯдЦ AI Processing: {question}")
        response = get_ai_response(question)

        return jsonify({"question": question, "answer": response})
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ЁЯФ╣ Root Route - рд╣реЗрд▓реНрде рдЪреЗрдХ
@app.route("/", methods=["GET"])
def home():
    return jsonify({"message": "тЬЕ AI Model is Running with Context!"})

# ЁЯФ╣ Flask Server рдХреЛ рдмреИрдХрдЧреНрд░рд╛рдЙрдВрдб рдореЗрдВ рдЪрд▓рд╛рдиреЗ рдХреЗ рд▓рд┐рдП
def run_flask():
    app.run(host="0.0.0.0", port=5000)

if __name__ == "__main__":
    threading.Thread(target=run_flask).start()
